# Action Localization Knowledge for ActionFormer

This document contains knowledge examples for temporal action localization tasks using ActionFormer.

## Example 1: Temporal Action Localization with ActionFormer

```json
{"data": "### Detailed Steps for Data Manipulation and Analysis in Temporal Action Localization\n\n#### 1. Dataset Retrieval and Preparation\n\n**Objective**: Build an ActionFormer model for temporal action localization in videos.\n\n**Dataset Specifications**:\n- Video features extracted from backbone networks (e.g., I3D, TSP, SlowFast)\n- Temporal annotations with start/end times and action classes\n- Common datasets: THUMOS14, ActivityNet, Ego4D, EPIC-KITCHENS\n\n**Source**:\n- The dataset will be provided by the user or retrieved from standard benchmarks.\n- Ensure video features are pre-extracted and stored in NumPy or similar format.\n\n**Steps**:\n1. **Verify Dataset Format**: Ensure the dataset contains:\n   - Feature files (e.g., .npy files with temporal features)\n   - Annotation files (JSON format with temporal boundaries and labels)\n   - Proper directory structure matching ActionFormer requirements\n2. **Check Feature Dimensions**: Verify feature dimension matches model configuration (e.g., 2048 for I3D, 400 for SlowFast)\n\n**Reason**: Proper dataset format is crucial for ActionFormer training. The model expects pre-extracted features and temporal annotations.\n\n#### 2. Data Preprocessing\n\n**Feature Normalization**:\n- **Technique**: Standard normalization or per-video normalization.\n- **Steps**: Features are typically already normalized during extraction.\n\n**Temporal Augmentation**:\n- **Techniques**:\n  1. **Random Cropping**: Crop video segments with specified ratios (e.g., [0.9, 1.0])\n  2. **Truncation**: Apply truncation threshold (e.g., 0.5) to handle long videos\n  3. **Max Sequence Length**: Limit maximum sequence length to prevent memory issues\n\n**Feature Engineering**:\n- **Steps**:\n  1. **Temporal Stride**: Adjust feature temporal stride to control resolution\n  2. **Window Size**: Configure temporal window size for feature aggregation\n\n**Reason**: Temporal augmentation improves model robustness and generalization to videos of varying lengths.\n\n#### 3. Model Configuration\n\n**IMPORTANT: For ActionFormer tasks, DO NOT write model code from scratch.**\n\n**Instead, use the `train_actionformer_wrapper.py` script:**\n- **Reason**: ActionFormer has a complex architecture with FPN, multi-scale processing, and specialized losses. Writing it from scratch is error-prone and time-consuming.\n- **Approach**: Use the wrapper script which handles configuration and training automatically.\n\n**Wrapper Usage**:\n```bash\npython train_actionformer_wrapper.py \\\n    --config_template thumos_i3d \\\n    --data_path ./data/thumos \\\n    --learning_rate 0.0001 \\\n    --epochs 30 \\\n    --batch_size 2 \\\n    --output_name my_experiment\n```\n\n**Configuration Templates Available**:\n- `thumos_i3d`: For THUMOS14 dataset with I3D features\n- `anet_i3d`: For ActivityNet with I3D features\n- `ego4d_slowfast`: For Ego4D with SlowFast features\n- `epic_slowfast_verb`: For EPIC-KITCHENS verb recognition\n- `epic_slowfast_noun`: For EPIC-KITCHENS noun recognition\n\n**Reason**: The wrapper abstracts away complexity while maintaining full ActionFormer functionality.\n\n#### 4. Data Visualization\n\n**Techniques**:\n- **Temporal Annotations Plot**: Visualize ground truth action boundaries\n- **Feature Heatmaps**: Visualize feature activations over time\n- **Prediction Visualization**: Plot predicted vs ground truth action segments\n\n**Reason**: Visualization helps understand temporal patterns and evaluate model predictions.\n\n#### 5. Expected Outcomes\n\n**Quantitative**:\n- **mAP (mean Average Precision)**: Primary evaluation metric for action localization\n  - THUMOS14: Expected mAP@0.5 around 60-65%\n  - ActivityNet: Expected mAP around 35-37%\n- **Training Time**: Varies by dataset size (typically 2-8 hours on single GPU)\n\n**Qualitative**:\n- **Temporal Precision**: Model accurately localizes action boundaries\n- **Multi-Scale Detection**: Detects actions at various temporal scales\n- **Robust to Variations**: Handles videos of different lengths and frame rates\n\nBy following these steps and using the wrapper script, you can effectively train ActionFormer models for temporal action localization without writing complex model code.", "model": "### Detailed Steps for Modeling and Optimization\n\n#### 1. Model Selection: Use ActionFormer Wrapper\n\n**CRITICAL: DO NOT write ActionFormer model code from scratch.**\n\n**Rationale**:\n- ActionFormer is a complex temporal action localization model with:\n  - Feature Pyramid Networks (FPN) for multi-scale processing\n  - Specialized classification and regression heads\n  - Custom loss functions (focal loss, IoU loss)\n  - Complex post-processing with NMS\n- Writing this from scratch is error-prone and time-consuming\n- A wrapper script is provided for easy integration\n\n**Approach**: Use `train_actionformer_wrapper.py`\n\n**Steps**:\n1. **Identify Dataset Type**: Determine which config template to use\n   - THUMOS14 → `thumos_i3d`\n   - ActivityNet → `anet_i3d` or `anet_tsp`\n   - Ego4D → `ego4d_slowfast`\n   - EPIC-KITCHENS → `epic_slowfast_verb` or `epic_slowfast_noun`\n\n2. **Configure Hyperparameters**: Set appropriate hyperparameters via command-line arguments\n   - `--learning_rate`: Typically 0.0001 to 0.0002\n   - `--epochs`: 20-30 for THUMOS14, 15-20 for ActivityNet\n   - `--batch_size`: 2-4 depending on GPU memory\n   - `--weight_decay`: 0.05 (default works well)\n\n3. **Execute Training**: Run the wrapper script\n   ```bash\n   python train_actionformer_wrapper.py \\\n       --config_template thumos_i3d \\\n       --data_path ./data/thumos \\\n       --learning_rate 0.0001 \\\n       --epochs 30 \\\n       --batch_size 2\n   ```\n\n4. **Monitor Training**: The wrapper will output:\n   - Training progress with loss values\n   - Validation metrics during training\n   - Final mAP score: `FINAL_mAP: XX.XX`\n\n#### 2. Hyperparameter Optimization\n\n**Key Hyperparameters and Optimal Ranges**:\n\n- **Learning Rate**: \n  - Range: [0.00005, 0.0002]\n  - Optimal: 0.0001 for most datasets\n  - Effect: Higher LR speeds training but may be unstable\n\n- **Batch Size**:\n  - Range: [1, 4]\n  - Optimal: 2 for 11GB GPU, 4 for 24GB GPU\n  - Effect: Larger batch improves gradient estimates\n\n- **Epochs**:\n  - Range: [15, 40]\n  - Optimal: 25-30 for THUMOS14, 15-20 for ActivityNet\n  - Effect: More epochs may overfit on smaller datasets\n\n- **Weight Decay**:\n  - Range: [0.01, 0.1]\n  - Optimal: 0.05\n  - Effect: Regularization to prevent overfitting\n\n**Optimization Strategy**:\n1. Start with default values from config templates\n2. If mAP is low, try increasing learning rate slightly\n3. If training is unstable, decrease learning rate or increase weight decay\n4. Adjust epochs based on validation performance plateau\n\n#### 3. Model Characteristics\n\n**Computation Complexity**:\n- **Parameters**: ~10-20M depending on backbone\n- **FLOPs**: ~50-100 GFLOPs per video\n- **Memory**: ~8-10GB GPU memory for batch size 2\n\n**Training Time**:\n- THUMOS14: ~2-3 hours on RTX 3090\n- ActivityNet: ~6-8 hours on RTX 3090\n- Ego4D: ~12-16 hours on RTX 3090\n\n**Inference Speed**:\n- ~5-10 videos per second on single GPU\n- Real-time capable for short videos\n\n#### 4. Expected Performance\n\n**Top-3 Configuration Candidates**:\n\n1. **THUMOS14 with I3D features**\n   - **mAP@0.3**: ~68%\n   - **mAP@0.4**: ~65%\n   - **mAP@0.5**: ~61%\n   - **Average mAP**: ~58%\n   - **Training Time**: 2.5 hours\n   - **Best Use**: General temporal action localization\n\n2. **ActivityNet with TSP features**\n   - **mAP@0.5**: ~38%\n   - **mAP@0.75**: ~35%\n   - **mAP@0.95**: ~22%\n   - **Average mAP**: ~36%\n   - **Training Time**: 7 hours\n   - **Best Use**: Long, untrimmed videos\n\n3. **Ego4D with SlowFast features**\n   - **mAP@0.1**: ~42%\n   - **mAP@0.2**: ~38%\n   - **mAP@0.3**: ~34%\n   - **Average mAP**: ~33%\n   - **Training Time**: 14 hours\n   - **Best Use**: Egocentric video understanding\n\n**Evaluation Metrics**:\n- **mAP**: Mean Average Precision at various IoU thresholds\n- **AR (Average Recall)**: Complementary metric for localization quality\n\n**Important Notes**:\n- Always use the wrapper script for ActionFormer\n- The wrapper handles all complexity internally\n- Focus on hyperparameter tuning rather than model architecture\n- Monitor `FINAL_mAP` in output for model performance\n\nBy using the wrapper approach, you can achieve state-of-the-art temporal action localization results without dealing with implementation complexity.", "pass": true}
```

## Example 2: Action Recognition vs Action Localization

When a user asks for \"action localization\", \"temporal action detection\", or \"action spotting\", they want **ActionFormer**, NOT simple action classification.

**Key Differences**:
- **Action Recognition**: Classify entire video (use ResNet, I3D, SlowFast directly)
- **Action Localization**: Find when actions occur in video (use ActionFormer wrapper)

**Decision Guide**:
- User mentions \"when\", \"locate\", \"temporal\", \"boundaries\" → Use ActionFormer wrapper
- User mentions only \"classify\", \"recognize\" entire video → Use standard classification

## Example 3: Quick Start Command

For most temporal action localization tasks:

```bash
# Step 1: Prepare your data in the required format
# - Feature files: .npy format
# - Annotations: JSON with start/end times

# Step 2: Run wrapper with appropriate template
python train_actionformer_wrapper.py \
    --config_template thumos_i3d \
    --data_path /path/to/your/data \
    --learning_rate 0.0001 \
    --epochs 30 \
    --batch_size 2 \
    --output_name my_experiment

# Step 3: Monitor output for FINAL_mAP score
```

**Remember**: NEVER write ActionFormer model code from scratch. Always use the wrapper script.
